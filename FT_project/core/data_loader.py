from config import settings
from langchain.schema import Document
from datasets import load_dataset
from pathlib import Path
from typing import List
from config.logger_config import configure_console_logger

logger = configure_console_logger(__name__)

def _load_hf_dataset() -> List[Document]:
    """æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†"""
    
    # ä»é…ç½®è¯»å–å‚æ•°
    cfg = settings.KNOWLEDGE_DATASET_CONFIG
    logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®é›† {cfg['dataset_name']}")
    
    try:
        # è‡ªåŠ¨æ£€æµ‹æœ¬åœ°ç¼“å­˜è·¯å¾„
        cache_dir = Path(settings.DATA_CACHE_DIR) 
        raw_data = load_dataset(
            cfg['dataset_name'], 
            cfg['config_name'],
            split=cfg['split'],
            cache_dir=str(cache_dir)
        )
    except Exception as e:
        logger.error(f"ğŸ”¥ æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
        return []

    # æœ‰æ•ˆæ€§æ ¡éªŒï¼ˆåŠ¨æ€æ£€æŸ¥å­—æ®µï¼‰
    required_columns = cfg['text_columns'] + [cfg['id_column']]
    valid_items = [
        item for item in raw_data 
        if all(k in item for k in required_columns)
    ]
    invalid_count = len(raw_data) - len(valid_items)
    
    logger.info(f"ğŸ“Š åŠ è½½å®Œæˆ: æœ‰æ•ˆ {len(valid_items)}æ¡, è·³è¿‡æ— æ•ˆ {invalid_count}æ¡")
    
    # ç»„åˆå¤šä¸ªæ–‡æœ¬å­—æ®µ
    return [
        Document(
            page_content= "\n".join(str(item[col]) for col in cfg['text_columns']),
            metadata={
                "doc_id": item[cfg['id_column']],
                "source": cfg['dataset_name']
            }
        ) for item in valid_items
    ]